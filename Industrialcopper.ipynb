{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly as px\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\sasim\\Desktop\\Industrial_Copper_Modeling\\Copper_Set.xlsx - Result 1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking null values in this Data Frame\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the unique values of the all columns\n",
    "df.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the datatypes\n",
    "# item_date, delivery date, quantity tons\n",
    "\n",
    "df[\"quantity tons\"] = pd.to_numeric(df[\"quantity tons\"],errors=\"coerce\")\n",
    "df[\"item_date_1\"]     = pd.to_datetime(df[\"item_date\"],format=\"%Y%m%d\", errors=\"coerce\").dt.date\n",
    "df[\"delivery_date_1\"] = pd.to_datetime(df[\"delivery date\"],format=\"%Y%m%d\", errors=\"coerce\").dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"material_ref\"] = df[\"material_ref\"].apply(lambda x: np.nan if str(x).startswith(\"00000\") else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"material_ref\" have a maximum null values (55%) so, we want to drop the column\n",
    "# And id is a unique values so we want to drop the column\n",
    "\n",
    "df.drop(columns = [\"id\",\"material_ref\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantity tons and selling_price have a negative values, This is impossible,\n",
    "# so we need to replace the negative values\n",
    "\n",
    "#converting the negative values into the null values\n",
    "df[\"quantity tons\"]=df[\"quantity tons\"].apply(lambda x: np.nan if x<0 else x)\n",
    "df[\"selling_price\"]=df[\"selling_price\"].apply(lambda x: np.nan if x<0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the null values using mean(),median() and mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object columns and mode method\n",
    "\n",
    "df[\"item_date_1\"].fillna(df[\"item_date_1\"].mode().iloc[0],inplace=True)\n",
    "df[\"delivery_date_1\"].fillna(df[\"delivery_date_1\"].mode().iloc[0],inplace=True)\n",
    "df[\"status\"].fillna(df[\"status\"].mode().iloc[0],inplace=True)\n",
    "df[\"item_date\"].fillna(df[\"item_date_1\"].mode().iloc[0],inplace=True)\n",
    "df[\"delivery date\"].fillna(df[\"delivery date\"].mode().iloc[0],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numarical column and median()\n",
    "\n",
    "df[\"quantity tons\"].fillna(df[\"quantity tons\"].median(),inplace=True)\n",
    "df[\"customer\"].fillna(df[\"customer\"].median(),inplace=True)\n",
    "df[\"country\"].fillna(df[\"country\"].median(),inplace=True)\n",
    "df[\"application\"].fillna(df[\"application\"].median(),inplace=True)\n",
    "df[\"thickness\"].fillna(df[\"thickness\"].median(),inplace=True)\n",
    "df[\"selling_price\"].fillna(df[\"selling_price\"].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the catagorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"status\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"status\"]= df[\"status\"].map({'Won':1, 'Draft':2, 'To be approved':3, 'Lost':0, 'Not lost for AM':4,\n",
    "                                'Wonderful':5, 'Revised':6, 'Offered':7, 'Offerable':8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 0, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"status\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"item type\"] = OrdinalEncoder().fit_transform(df[[\"item type\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 6., 3., 1., 2., 0., 4.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"item type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Industrial_Copper_proper.csv\",index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the skewness using (Log Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detecting the skewed columns using plot\n",
    "def plot(df,column):\n",
    "  #distplot\n",
    "  plt.figure(figsize=(15,4))\n",
    "  plt.subplot(1,3,1)\n",
    "  sns.distplot(df[column])\n",
    "  plt.title(\"distplot for\"+\" \"+column)\n",
    "\n",
    "  #histogram plot\n",
    "\n",
    "  plt.subplot(1,3,2)\n",
    "  sns.histplot(df, x= column, kde= True, bins=30,color=\"salmon\")\n",
    "  plt.title(\"histogram plot for\"+\" \"+column)\n",
    "\n",
    "  #boxplot\n",
    "\n",
    "  plt.subplot(1,3,3)\n",
    "  sns.boxplot(df, x=column)\n",
    "  plt.title(\"Box plot for\"+\" \"+column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_columns=['quantity tons', 'customer', 'country', 'status',\n",
    "                'item type', 'application', 'thickness', 'width', 'product_ref',\n",
    "                'selling_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in skewed_columns:\n",
    "  plot(df,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skewed columns:\n",
    "# 1.quantity tons\n",
    "# 2.customer\n",
    "# 3.thickness\n",
    "# 4.selling_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"quantity_tons_log\"]= np.log(df1[\"quantity tons\"])\n",
    "df1[\"customer_log\"]= np.log(df1[\"customer\"])\n",
    "df1[\"thickness_log\"]= np.log(df1[\"thickness\"])\n",
    "df1[\"selling_price_log\"]= np.log(df1[\"selling_price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, column):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    plt.figure(figsize=(15, 4))\n",
    "\n",
    "    # Clean data (drop NaNs and infs for plotting)\n",
    "    data = df[column].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    # Histogram + KDE\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.histplot(data, kde=True, bins=30, color=\"skyblue\")\n",
    "    plt.title(f\"Histogram + KDE for {column}\")\n",
    "\n",
    "    # Boxplot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.boxplot(x=data, color=\"salmon\")\n",
    "    plt.title(f\"Boxplot for {column}\")\n",
    "\n",
    "    # Show skewness\n",
    "    skewness = data.skew()\n",
    "    plt.suptitle(f\"Skewness: {skewness:.2f}\", fontsize=12, fontweight=\"bold\", color=\"darkblue\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skwed_columns_2=[\"quantity_tons_log\",\"customer_log\",\"thickness_log\",\"selling_price_log\"]\n",
    "for i in skwed_columns_2:\n",
    "  plot(df1,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Handling - Interquartile Range(IQR) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier(df,column):\n",
    "  q1= df[column].quantile(0.25)\n",
    "  q3= df[column].quantile(0.75)\n",
    "\n",
    "  iqr= q3-q1\n",
    "\n",
    "  upper_threshold= q3 + (1.5*iqr)\n",
    "  lower_threshold= q1 - (1.5*iqr)\n",
    "\n",
    "  df[column]= df[column].clip(lower_threshold, upper_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Ex: lower threshold = 5 and upper threshold = 20)\n",
    "# above upper threshold values (>20) are converted to upper threshold value (20) in features\n",
    "# below lower threshold values (<5)  are converted to lower threshold value (5)  in features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_columns= ['quantity_tons_log', 'customer_log', 'thickness_log','selling_price_log','width','application']\n",
    "for i in outlier_columns:\n",
    "  outlier(df2,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in outlier_columns:\n",
    "  plot(df2,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Droping the unwanted skewed columns\n",
    "#Skewed columns:\n",
    "# 1.quantity tons\n",
    "# 2.customer\n",
    "# 3.thickness\n",
    "# 4.selling_price\n",
    "\n",
    "df3= df2.drop(columns=[\"quantity tons\",\"customer\",\"thickness\",\"selling_price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the correlation with using the heatmap\n",
    "\n",
    "corr= df3.drop(columns=[\"item_date\",\"delivery date\",\"item_date_1\",\"delivery_date_1\"]).corr()\n",
    "plt.figure(figsize=(10,3))\n",
    "sns.heatmap(corr, annot= True, fmt=\"0.2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the high correlations are \"-0.42\", \"0.40\", \"-0.32\", \"-0.20\",\"0.23\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrong delivery date handling using ML prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4= df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the datatype (object to datetime format)\n",
    "df4[\"delivery_date_1\"]= pd.to_datetime(df4[\"delivery_date_1\"])\n",
    "df4[\"item_date_1\"]= pd.to_datetime(df4[\"item_date_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying the difference of the \"delivery date\" and the \"item date\"\n",
    "df4[\"date_differ\"]= (df4[\"delivery_date_1\"]-df4[\"item_date_1\"]).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[\"date_differ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# some values have a negative values\n",
    "# it's mean the \"delivery date provides , the previous date then the \"item date\"\n",
    "# so this is not possible , so we want to predic the delivery date for the some datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the another 3 columns using the \"item_date_1\"\n",
    "# it is usefull for the delivery date prediction\n",
    "df4[\"item_date_day\"]= df4[\"item_date_1\"].dt.day\n",
    "df4[\"item_date_month\"]= df4[\"item_date_1\"].dt.month\n",
    "df4[\"item_date_year\"]= df4[\"item_date_1\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the posive values(pv) dataframe and negative values(nv) dataframe based on the \"date_differ\" column\n",
    "df4_pv= df4[df4[\"date_differ\"]>=0]\n",
    "df4_pv.reset_index(drop= True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df4_pv.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_nv= df4[df4[\"date_differ\"]<0]\n",
    "df4_nv.reset_index(drop= True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_nv.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to create the model for delivery date prediction\n",
    "# importing the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_pv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best algorithm for the \"delivery date\" prediction\n",
    "\n",
    "def accuracy_date_prediction(df, algorithm):\n",
    "  x= df.drop(columns=[\"item_date_1\", \"delivery_date_1\", \"date_differ\"])\n",
    "  y= df[\"date_differ\"]\n",
    "\n",
    "  #teain test splitting\n",
    "  x_train, x_test, y_train, y_test= train_test_split(x,y, test_size= 0.2, random_state=42)\n",
    "\n",
    "  model= algorithm().fit(x_train,y_train)\n",
    "  y_pred= model.predict(x_test)\n",
    "\n",
    "  #checking the accuracy score\n",
    "  mse= mean_squared_error(y_test, y_pred)\n",
    "  rmse= np.sqrt(mse)\n",
    "  mae= mean_absolute_error(y_test,y_pred)\n",
    "  r2= r2_score(y_test, y_pred)\n",
    "\n",
    "  metrics={\"R2_score\":r2,\n",
    "           \"Mean_squared_error\":mse,\n",
    "           \"Root_mean_squared_error\":rmse,\n",
    "           \"Mean_absolute_error\":mae,\n",
    "           }\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_date_prediction(df4_pv,DecisionTreeRegressor))\n",
    "print(accuracy_date_prediction(df4_pv,RandomForestRegressor))\n",
    "print(accuracy_date_prediction(df4_pv,ExtraTreesRegressor))\n",
    "print(accuracy_date_prediction(df4_pv,GradientBoostingRegressor))\n",
    "print(accuracy_date_prediction(df4_pv,AdaBoostRegressor))\n",
    "print(accuracy_date_prediction(df4_pv,XGBRegressor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest algorithm is low bias and reduce overfitting compared to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for the RandomForest\n",
    "\n",
    "def RandomForest(train_df, test_df):\n",
    "\n",
    "  x= train_df.drop(columns=[\"item_date_1\", \"delivery_date_1\", \"date_differ\"])\n",
    "  y= train_df[\"date_differ\"]\n",
    "\n",
    "  #train test splitting\n",
    "  x_train, x_test, y_train, y_test= train_test_split(x,y, test_size= 0.2, random_state=42)\n",
    "  model= RandomForestRegressor().fit(x_train, y_train)\n",
    "\n",
    "  data= test_df.drop(columns=[\"item_date_1\", \"delivery_date_1\", \"date_differ\"])\n",
    "\n",
    "  y_pred=model.predict(data)\n",
    "\n",
    "  return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_difference= RandomForest(df4_pv,df4_nv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the \"date_differ\" datatype float into int\n",
    "date_difference_1= []\n",
    "for i in date_difference:\n",
    "  dd= int(round(i,0))\n",
    "  date_difference_1.append(dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_nv[\"date_differ\"]= pd.DataFrame(date_difference_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_nv.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the delivery date using \"item_date_1\" and \"date_differ\"\n",
    "\n",
    "def find_delivery_date(item_date, date_differ):\n",
    "  date= item_date + pd.to_timedelta(date_differ,unit= \"D\")\n",
    "  return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_nv[\"delivery_date_1\"]= find_delivery_date(df4_nv[\"item_date_1\"],df4_nv[\"date_differ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concadinating the two dataframes(df4_pv,df4_nv) based on the rows\n",
    "df_final=pd.concat([df4_pv,df4_nv],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create the three new columns using the \"delivery_date_1\"\n",
    "df_final['delivery_date_day']= df_final[\"delivery_date_1\"].dt.day\n",
    "df_final['delivery_date_month']= df_final[\"delivery_date_1\"].dt.month\n",
    "df_final['delivery_date_year']= df_final[\"delivery_date_1\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping the unwanted columns\n",
    "df_final.drop(columns=[\"item_date\",\"delivery date\",\"item_date_1\",\"delivery_date_1\",\"date_differ\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saveing the dataframe\n",
    "#df_final.to_csv(\"Industrial_Copper_Colab.csv\",index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the saved \"csv\" file\n",
    "df_final= pd.read_csv(\"/content/Industrial_Copper_Colab.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This DataFrame don't have any null values and catagorical columns\n",
    "# so our dataset is ready to the ML prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Method - Predict (Won/Lose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,auc,roc_curve,confusion_matrix,classification_report\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class= df_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the status column only want to be a (won& loss)\n",
    "df_c= df_class[(df_class[\"status\"] == 1) | (df_class[\"status\"] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c[\"status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a imbalanced data\n",
    "# so we want to resampling the data by usin the \"SMOTETomek\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= df_c.drop(columns=[\"status\"],axis=1)\n",
    "y= df_c[\"status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new,y_new= SMOTETomek().fit_resample(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new.shape,y_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the data is balanced\n",
    "# so, we continue the ML prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best algorithm for the classification prediction\n",
    "\n",
    "def accuracy_checking(x_data, y_data, algorithm):\n",
    "  #train test splitting\n",
    "  x_train, x_test, y_train, y_test= train_test_split(x_data, y_data, test_size= 0.2, random_state=42)\n",
    "\n",
    "  model= algorithm().fit(x_train, y_train)\n",
    "\n",
    "  y_pred_train= model.predict(x_train)\n",
    "  y_pred_test= model.predict(x_test)\n",
    "\n",
    "  #checking the accuracy_score\n",
    "  accuracy_train= accuracy_score(y_train, y_pred_train)\n",
    "  accuracy_test= accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "  metrics={\"Algorithm\": algorithm.__name__,\n",
    "           \"Accuracy_Train\": accuracy_train,\n",
    "           \"Accuracy_Test\": accuracy_test}\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_checking(x,y,DecisionTreeClassifier))\n",
    "print(accuracy_checking(x,y,RandomForestClassifier))\n",
    "print(accuracy_checking(x,y,ExtraTreesClassifier))\n",
    "print(accuracy_checking(x,y,AdaBoostClassifier))\n",
    "print(accuracy_checking(x,y,GradientBoostingClassifier))\n",
    "print(accuracy_checking(x,y,XGBClassifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_checking(x_new,y_new,DecisionTreeClassifier))\n",
    "print(accuracy_checking(x_new,y_new,RandomForestClassifier))\n",
    "print(accuracy_checking(x_new,y_new,ExtraTreesClassifier))\n",
    "print(accuracy_checking(x_new,y_new,AdaBoostClassifier))\n",
    "print(accuracy_checking(x_new,y_new,GradientBoostingClassifier))\n",
    "print(accuracy_checking(x_new,y_new,XGBClassifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we got good accuracy after oversampling\n",
    "# ExtraTreesClassifier and RandomForestClassifier both have good testing accuracy, but in training accuracy is overfitting.\n",
    "# so we want the reduce the overfitting.\n",
    "# RandomForestClassifier is good interpretability, so i select the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the high accuracy using Hyperparameter Tuning method with using of the GridsearchCV\n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(x_new,y_new, test_size= 0.2, random_state=42)\n",
    "\n",
    "parameters= {\"max_depth\": [2,5,10,20],\n",
    "             \"min_samples_split\": [2,5,10],\n",
    "             \"min_samples_leaf\": [1,2,4],\n",
    "             \"max_features\": ['sqrt', 'log2', None]}\n",
    "\n",
    "gridsearch= GridSearchCV(estimator= RandomForestClassifier(), param_grid= parameters, cv= 5, n_jobs= -1)\n",
    "gridsearch.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the best Hypertuning paramers in the ,\n",
    "# RandomForest algorithm and check the accuracy for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test= train_test_split(x_new, y_new, test_size= 0.2, random_state= 42)\n",
    "\n",
    "model= RandomForestClassifier(max_depth=20, max_features= None, min_samples_leaf=1, min_samples_split=2).fit(x_train,y_train)\n",
    "\n",
    "y_pred_train= model.predict(x_train)\n",
    "y_pred_test= model.predict(x_test)\n",
    "\n",
    "#checking the accuracy_score for train and test\n",
    "\n",
    "accuracy_train= accuracy_score(y_train, y_pred_train)\n",
    "accuracy_test= accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Accuracy score for Train and Test\")\n",
    "print(\"----------------------------------\")\n",
    "print(\"Accuracy_Train: \",accuracy_train)\n",
    "print(\"Accuracy_Test: \",accuracy_test)\n",
    "print(\"  \")\n",
    "#confution matrics and the classification report for test\n",
    "\n",
    "print(\"Confution_matrix for Test\")\n",
    "print(\"--------------------------\")\n",
    "print(confusion_matrix(y_true= y_test, y_pred= y_pred_test))\n",
    "print(\" \")\n",
    "print(\"Classification_report for Test\")\n",
    "print(\"-------------------------------\")\n",
    "print(classification_report(y_true= y_test, y_pred= y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the training accuracy overfitting reduced. so now model will predict effectively for unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC)\n",
    "\n",
    "FP,TP,threshold= roc_curve(y_true= y_test, y_score=y_pred_test)\n",
    "print(threshold)\n",
    "print(FP)\n",
    "print(TP)\n",
    "print(\" \")\n",
    "auc_curve= auc(x=FP,y=TP)\n",
    "print(\"auc_curve:\",auc_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot for roc and auc curve\n",
    "roc_point= {\"ROC Curve (area)\":round(auc_curve,2)}\n",
    "plt.plot(FP,TP,label= roc_point)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.1])\n",
    "plt.xlabel(\"False Positive\")\n",
    "plt.ylabel(\"True Positive\")\n",
    "plt.plot([0,1],[0,1],\"k--\")\n",
    "plt.legend(loc= \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = np.array([[30153963, 30, 6, 28, 952, 628377, 5.9, -0.96, 6.46, 1,4,2021,1,1,2021]])\n",
    "y_pred_user= model.predict(user_data)\n",
    "if y_pred_user == 1:\n",
    "    print(\"Won\")\n",
    "else:\n",
    "    print(\"Lose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model using the pickle\n",
    "\n",
    "with open(\"Classification_model.pkl\",\"wb\") as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/content/Classification_model.pkl\",\"rb\") as f1:\n",
    "  class_model= pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = np.array([[77.0,3.0,10.0,1500.0,164141591,3.677655,17.222226,0.000000,7.110696,1,4,2021,1,8,2021]])\n",
    "y_pred_user= class_model.predict(user_data)\n",
    "\n",
    "if y_pred_user == 1:\n",
    "    print(\"Won\")\n",
    "else:\n",
    "    print(\"Lose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr= df_final.corr()\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.heatmap(corr,annot= True, fmt=\"0.2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Method - Predict the Selling Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regg= df_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best ML model to predict the selling price\n",
    "\n",
    "def accuracy_regressor(df,algorithm):\n",
    "\n",
    "  x= df.drop(columns=[\"selling_price_log\"], axis=1)\n",
    "  y= df[\"selling_price_log\"]\n",
    "\n",
    "  #train test splitting\n",
    "  x_train,x_test, y_train, y_test= train_test_split(x,y, test_size= 0.2, random_state=42)\n",
    "  model= algorithm().fit(x_train, y_train)\n",
    "\n",
    "  y_pred_train= model.predict(x_train)\n",
    "  y_pred_test= model.predict(x_test)\n",
    "\n",
    "  r2_train= r2_score(y_train, y_pred_train)\n",
    "  r2_test= r2_score(y_test, y_pred_test)\n",
    "\n",
    "  metrics={\"Algorithm\":algorithm.__name__,\n",
    "           \"R2_Train\": r2_train,\n",
    "           \"R2_Test\": r2_test}\n",
    "\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_regressor(df_regg, DecisionTreeRegressor))\n",
    "print(accuracy_regressor(df_regg, RandomForestRegressor))\n",
    "print(accuracy_regressor(df_regg, ExtraTreesRegressor))\n",
    "print(accuracy_regressor(df_regg, AdaBoostRegressor))\n",
    "print(accuracy_regressor(df_regg, GradientBoostingRegressor))\n",
    "print(accuracy_regressor(df_regg, XGBRegressor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExtraTreesClassifier and RandomForestClassifier both have good testing accuracy, but in training accuracy is overfitting.\n",
    "# RandomForestClassifier is good interpretability, so i select the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= df_regg.drop(columns=[\"selling_price_log\"], axis=1)\n",
    "y= df_regg[\"selling_price_log\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y, test_size= 0.2, random_state= 42)\n",
    "\n",
    "parameters_r= {\"max_depth\": [2,4,10,20],\n",
    "               \"min_samples_split\": [2,5,10],\n",
    "               \"min_samples_leaf\": [1,2,4],\n",
    "               \"max_features\": [\"sqrt\",\"log2\",None]}\n",
    "\n",
    "gridsearch_r= GridSearchCV(estimator= RandomForestRegressor(), param_grid= parameters_r, cv= 5,n_jobs=-1)\n",
    "gridsearch_r.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_r.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_r.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_r.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the parameters and check the accuracy for both training and testing & overfitting\n",
    "\n",
    "x = df_regg.drop(columns=['selling_price_log'], axis=1)\n",
    "y = df_regg['selling_price_log']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "\n",
    "model_r = RandomForestRegressor(max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2).fit(x_train, y_train)\n",
    "y_pred_train = model_r.predict(x_train)\n",
    "y_pred_test = model_r.predict(x_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "r2_train, r2_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the selling price with hypertuning parameters and calculate the accuracy using metrics\n",
    "\n",
    "x = df_regg.drop(columns=['selling_price_log'], axis=1)\n",
    "y = df_regg['selling_price_log']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "\n",
    "model_r = RandomForestRegressor(max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2).fit(x_train, y_train)\n",
    "y_pred = model_r.predict(x_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "metrics_r = {'R2': r2,\n",
    "           'Mean Absolute Error': mae,\n",
    "           'Mean Squared Error': mse,\n",
    "           'Root Mean Squared Error': rmse}\n",
    "\n",
    "metrics_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = np.array([[30202938,25,1,5,41,1210,1668701718,6.6,-0.2,1,4,2021,1,4,2021]])\n",
    "y_pred = model_r.predict(user_data)\n",
    "print(\"Predicted selling price with Log: \",y_pred[0])\n",
    "print(\"Predicted selling price without Log: \",np.exp(y_pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Regression_Model.pkl\",\"wb\") as r:\n",
    "  pickle.dump(model_r,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Regression_Model.pkl\",\"rb\") as r1:\n",
    "  regg_model=pickle.load(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = np.array([[28.0,1,5.0,10.0,1500.0,1670798778,3.991779,17.221905,0.693147,1,4,2021,1,7,2021]])\n",
    "y_pred = regg_model.predict(user_data)\n",
    "print(\"Predicted selling price with Log: \",y_pred[0])\n",
    "print(\"Predicted selling price without Log: \",np.exp(y_pred[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
